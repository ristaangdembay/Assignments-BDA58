{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4246a9ce",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing (NLP): Basics of Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65116fe",
   "metadata": {},
   "source": [
    "Three Domains AI: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb62bd4",
   "metadata": {},
   "source": [
    "- Computer Vision [Image and Video - Image Processing -> CNN and RNN]\n",
    "- Natural Language Processing [Text and Audio] Chatbots, Alexa. Siri\n",
    "- Statistical Data [ CSV,Excel -> Recommendation & Prediction using historical data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f571df7",
   "metadata": {},
   "source": [
    "- Agenda:\n",
    "    - How NLP applications work [How chatbots work?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9182b",
   "metadata": {},
   "source": [
    "- NLP: Natural Language Processing\n",
    "    - Def: Natural Language Processing (NLP) is a domain of artificial intelligence (AI) focused on the interaction between computers and humans through natural language.\n",
    "    - Goal: The primary goal of NLP is to enable machines to understand, interpret, and generate human language in a way that is valuable.\n",
    "    - Applications: Applications of NLP include language translation (Google Translate), chatbots (like ChatGPT), sentiment analysis (identifying emotions in tweets or reviews), and much more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17a628",
   "metadata": {},
   "source": [
    "- Steps:\n",
    "    - Humans communicate with machine using text from any language [English] Ex: ChatGPT Prompts --> Machine takes these texts as inputs--> Text Processing is done by machine --> Why? -> it helps to  clean, structure, and convert raw text into a form that machines can easily interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb735e5",
   "metadata": {},
   "source": [
    "- Key Steps in Text Preprocessing\n",
    "    - 1. Tokenization\n",
    "    - 2. Lowercasing\n",
    "    - 3. Removing Stop Words\n",
    "    - 4. Removing Punctuation\n",
    "    - 5. Stemming and Lemmatization\n",
    "    - 6. Handling Numbers\n",
    "    - 7. Removing Special Characters\n",
    "    - 8. Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cca8e6",
   "metadata": {},
   "source": [
    "### Step 1. Tokenization:\n",
    "    - Def.: \n",
    "        - Tokenization is the process of breaking a text into individual components or \"tokens\" like words, phrases, or sentences. \n",
    "        - It serves as the foundation for all further text analysis.\n",
    "        \n",
    "    - Example:\n",
    "        - Text: \"Natural language processing is fascinating.\"\n",
    "\n",
    "        - Tokenized: [\"Natural\", \"language\", \"processing\", \"is\", \"fascinating\"]\n",
    "\n",
    "    - Scenario based question:\n",
    "        - In real-time, when analyzing product reviews, tokenizing the sentences allows you to understand each word individually.\n",
    "        - For instance, in a customer review like \"The laptop is fast and efficient,\" tokenization separates the words \"laptop,\" \"fast,\" and \"efficient,\" which can help identify the core sentiments.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "363e8ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing the required libraries\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "096848ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rista\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rista\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data files (stopwords, wordnet, etc.)\n",
    "u = nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76bee808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Reviews in English\n",
    "reviews = [\n",
    "    \"The movie was fantastic! The storyline kept me on the edge of my seat the entire time 🔥.\",\n",
    "    \"This restaurant has the best pasta I've ever had. The service was quick, but the ambiance could be better.\",\n",
    "    \"I'm disappointed with the phone's battery life. It drains way too fast, but the camera quality is gr8. I will give 10 out of 10 !\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab4df21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 1: Tokenization\n",
    "# 1.1 Define function to Tokenizing each review into individual words (tokens)\n",
    "def tokenize_review(rev):\n",
    "    tokens = nltk.word_tokenize(rev)\n",
    "    return tokens\n",
    "\n",
    "#tokenize_review(reviews) #TypeError: expected string or bytes-like object, got 'list'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a92fc00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Reviews:\n",
      "Review 1: ['The', 'movie', 'was', 'fantastic', '!', 'The', 'storyline', 'kept', 'me', 'on', 'the', 'edge', 'of', 'my', 'seat', 'the', 'entire', 'time', '🔥', '.']\n",
      "Review 2: ['This', 'restaurant', 'has', 'the', 'best', 'pasta', 'I', \"'ve\", 'ever', 'had', '.', 'The', 'service', 'was', 'quick', ',', 'but', 'the', 'ambiance', 'could', 'be', 'better', '.']\n",
      "Review 3: ['I', \"'m\", 'disappointed', 'with', 'the', 'phone', \"'s\", 'battery', 'life', '.', 'It', 'drains', 'way', 'too', 'fast', ',', 'but', 'the', 'camera', 'quality', 'is', 'gr8', '.', 'I', 'will', 'give', '10', 'out', 'of', '10', '!']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing each review\n",
    "tokenized_reviews = [tokenize_review(rev) for rev in reviews]\n",
    "print(\"Tokenized Reviews:\")\n",
    "for i, tokens in enumerate(tokenized_reviews):\n",
    "    print(f\"Review {i+1}:\", tokens)\n",
    "    #print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4560f1",
   "metadata": {},
   "source": [
    "punkt not found error occured. Hence, import nltk and download pinkt using the command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1518345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rista\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d6ea49",
   "metadata": {},
   "source": [
    "Rerun the step 1: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6783da",
   "metadata": {},
   "source": [
    "### Step 1: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "492d1d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Tokenization\n",
    "# 1.1 Define function to Tokenizing each review into individual words (tokens)\n",
    "def tokenize_review(rev):\n",
    "    tokens = nltk.word_tokenize(rev)\n",
    "    return tokens\n",
    "\n",
    "#tokenize_review(reviews) #TypeError: expected string or bytes-like object, got 'list'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d1eaa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Reviews:\n",
      "Review 1: ['The', 'movie', 'was', 'fantastic', '!', 'The', 'storyline', 'kept', 'me', 'on', 'the', 'edge', 'of', 'my', 'seat', 'the', 'entire', 'time', '🔥', '.']\n",
      "Review 2: ['This', 'restaurant', 'has', 'the', 'best', 'pasta', 'I', \"'ve\", 'ever', 'had', '.', 'The', 'service', 'was', 'quick', ',', 'but', 'the', 'ambiance', 'could', 'be', 'better', '.']\n",
      "Review 3: ['I', \"'m\", 'disappointed', 'with', 'the', 'phone', \"'s\", 'battery', 'life', '.', 'It', 'drains', 'way', 'too', 'fast', ',', 'but', 'the', 'camera', 'quality', 'is', 'gr8', '.', 'I', 'will', 'give', '10', 'out', 'of', '10', '!']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing each review\n",
    "tokenized_reviews = [tokenize_review(rev) for rev in reviews]\n",
    "print(\"Tokenized Reviews:\")\n",
    "for i, tokens in enumerate(tokenized_reviews):\n",
    "    print(f\"Review {i+1}:\", tokens)\n",
    "    #print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565c07c8",
   "metadata": {},
   "source": [
    "### Step 2: Lowercasing\n",
    "- Convert all the words to lowercase to maintain consistency. \n",
    "- Otherwise, words like \"Phone\" and \"phone\" might be treated as different.\n",
    "\n",
    "    - Example:\n",
    "    - Lowercased: ['the', 'movie', 'was', 'fantastic', '!', 'the', 'storyline', 'kept', 'me', 'on', 'the', 'edge', 'of', 'my', 'seat', 'the', 'entire', 'time', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccd6205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Lowercasing\n",
    "# Converting all tokens to lowercase for uniformity\n",
    "def lowercase_tokens(tokens):\n",
    "    return [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "440cb08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lowercased Reviews:\n",
      "['the', 'movie', 'was', 'fantastic', '!', 'the', 'storyline', 'kept', 'me', 'on', 'the', 'edge', 'of', 'my', 'seat', 'the', 'entire', 'time', '🔥', '.']\n",
      "['this', 'restaurant', 'has', 'the', 'best', 'pasta', 'i', \"'ve\", 'ever', 'had', '.', 'the', 'service', 'was', 'quick', ',', 'but', 'the', 'ambiance', 'could', 'be', 'better', '.']\n",
      "['i', \"'m\", 'disappointed', 'with', 'the', 'phone', \"'s\", 'battery', 'life', '.', 'it', 'drains', 'way', 'too', 'fast', ',', 'but', 'the', 'camera', 'quality', 'is', 'gr8', '.', 'i', 'will', 'give', '10', 'out', 'of', '10', '!']\n"
     ]
    }
   ],
   "source": [
    "lowercased_reviews = [lowercase_tokens(tokens) for tokens in tokenized_reviews]\n",
    "print(\"\\nLowercased Reviews:\")\n",
    "for i, tokens in enumerate(lowercased_reviews):\n",
    "    # print(f\"Review {i+1}:\", tokens)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78785d20",
   "metadata": {},
   "source": [
    "### Step 3: Removing Stop Words\n",
    "- In Hindi and English, common words like “aur” (and), “hai” (is), “bhi” (also), “the,” and “is” don’t add much value to the analysis. \n",
    "- Remove these stop words to focus on the more meaningful terms in the reviews.\n",
    "\n",
    "- Example:\n",
    "    - Without stop words: ['movie', 'fantastic', '!', 'storyline', 'kept', 'edge', 'seat', 'entire', 'time', '.']\n",
    "- Now, the review was boiled down to the most relevant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "864b22ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Removing Stop Words\n",
    "# Stop words are common words like 'is', 'and', 'the' that don't add significant meaning to the text\n",
    "stop_words = set(stopwords.words('english')) # Combining English and Hindi stopwords\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32cf4a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reviews after Removing Stop Words:\n",
      "Review 1: ['movie', 'fantastic', '!', 'storyline', 'kept', 'edge', 'seat', 'entire', 'time', '🔥', '.']\n",
      "Review 2: ['restaurant', 'best', 'pasta', \"'ve\", 'ever', '.', 'service', 'quick', ',', 'ambiance', 'could', 'better', '.']\n",
      "Review 3: [\"'m\", 'disappointed', 'phone', \"'s\", 'battery', 'life', '.', 'drains', 'way', 'fast', ',', 'camera', 'quality', 'gr8', '.', 'give', '10', '10', '!']\n"
     ]
    }
   ],
   "source": [
    "filtered_reviews = [remove_stop_words(tokens) for tokens in lowercased_reviews]\n",
    "print(\"\\nReviews after Removing Stop Words:\")\n",
    "for i, tokens in enumerate(filtered_reviews):\n",
    "    print(f\"Review {i+1}:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e68f4cf",
   "metadata": {},
   "source": [
    "### Step 4: Removing Punctuation and Emojis\n",
    "Next, we remove unnecessary punctuation marks and emojis. While the fire emoji (🔥) might express enthusiasm, the machine learning model wouldn't understand it unless programmed specifically for emoji analysis.\n",
    "\n",
    "Example:\n",
    "Without punctuation and emojis: [\"phone\", \"mast\", \"battery\", \"life\", \"zabardast\", \"camera\", \"sahi\"]\n",
    "This step further cleaned up the data and made it ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ae94f",
   "metadata": {},
   "source": [
    "### clean_tokens = [token[output] for token[variable] in tokens[list] if[condition] token.isalnum()]  # Retain only alphanumeric tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02cb241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Removing Punctuation and Emojis\n",
    "# Removing punctuation and emojis from tokens\n",
    "def remove_punctuation_and_emojis(tokens):\n",
    "    clean_tokens = [token for token in tokens if token.isalnum()]  # Retain only alphanumeric tokens\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db70fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explanation -->     clean_tokens = [token for token in tokens if token.isalnum()]  # Retain only alphanumeric tokens\n",
    "\n",
    "clean_tokens = [\n",
    "    token         # Include the token\n",
    "    for token in tokens  # Loop through each token in the 'tokens' list\n",
    "    if token.isalnum()   # Check if the token is alphanumeric (removes punctuation and special characters)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12db2dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reviews after Removing Punctuation and Emojis:\n",
      "Review 1: ['movie', 'fantastic', 'storyline', 'kept', 'edge', 'seat', 'entire', 'time']\n",
      "Review 2: ['restaurant', 'best', 'pasta', 'ever', 'service', 'quick', 'ambiance', 'could', 'better']\n",
      "Review 3: ['disappointed', 'phone', 'battery', 'life', 'drains', 'way', 'fast', 'camera', 'quality', 'gr8', 'give', '10', '10']\n"
     ]
    }
   ],
   "source": [
    "cleaned_reviews = [remove_punctuation_and_emojis(tokens) for tokens in filtered_reviews]\n",
    "print(\"\\nReviews after Removing Punctuation and Emojis:\")\n",
    "for i, tokens in enumerate(cleaned_reviews):\n",
    "    print(f\"Review {i+1}:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7dc643",
   "metadata": {},
   "source": [
    "### Step 5: Stemming and Lemmatization\n",
    "- Goal: To get to the root of each word, apply stemming and lemmatization. \n",
    "- Stemming removes word endings, and \n",
    "- Lemmatization turns words into their base form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e940405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Stemming and Lemmatization\n",
    "# Stemming reduces words to their base form (e.g., 'running' -> 'run')\n",
    "# Lemmatization reduces words to their dictionary root form (e.g., 'running' -> 'run')\n",
    "\n",
    "# Using NLTK's PorterStemmer for stemming\n",
    "stemmer = PorterStemmer()\n",
    "def apply_stemming(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fc901d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemmed Reviews:\n",
      "Review 1: ['movi', 'fantast', 'storylin', 'kept', 'edg', 'seat', 'entir', 'time']\n",
      "Review 2: ['restaur', 'best', 'pasta', 'ever', 'servic', 'quick', 'ambianc', 'could', 'better']\n",
      "Review 3: ['disappoint', 'phone', 'batteri', 'life', 'drain', 'way', 'fast', 'camera', 'qualiti', 'gr8', 'give', '10', '10']\n"
     ]
    }
   ],
   "source": [
    "stemmed_reviews = [apply_stemming(tokens) for tokens in cleaned_reviews]\n",
    "print(\"\\nStemmed Reviews:\")\n",
    "for i, tokens in enumerate(stemmed_reviews):\n",
    "    print(f\"Review {i+1}:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63b7e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using NLTK's WordNetLemmatizer for lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def apply_lemmatization(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e0daea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized Reviews:\n",
      "Review 1: ['movie', 'fantastic', 'storyline', 'kept', 'edge', 'seat', 'entire', 'time']\n",
      "Review 2: ['restaurant', 'best', 'pasta', 'ever', 'service', 'quick', 'ambiance', 'could', 'better']\n",
      "Review 3: ['disappointed', 'phone', 'battery', 'life', 'drain', 'way', 'fast', 'camera', 'quality', 'gr8', 'give', '10', '10']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_reviews = [apply_lemmatization(tokens) for tokens in cleaned_reviews]\n",
    "print(\"\\nLemmatized Reviews:\")\n",
    "for i, tokens in enumerate(lemmatized_reviews):\n",
    "    print(f\"Review {i+1}:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6b02ed",
   "metadata": {},
   "source": [
    "### Step 6. Handling Numbers\n",
    "Numbers often don't carry significant meaning unless they represent quantities or rankings. You can choose to either remove, replace, or leave them as is.\n",
    "\n",
    "Example:\n",
    "Text: \"He bought 3 laptops for $1500.\"\n",
    "\n",
    "Without numbers: \"He bought laptops for .\"\n",
    "Replacing numbers with placeholders: \"He bought <NUM> laptops for <NUM>.\"\n",
    "In an e-commerce setting, this can help abstract numerical data, allowing the model to focus on the context rather than specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f3d1f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonum_rev(tokens): \n",
    "    nonum_token = [token for token in tokens if not token.isdigit()]\n",
    "    return nonum_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c4d35b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Review with handled Numbers: \n",
      "Review 1: ['movie', 'fantastic', 'storyline', 'kept', 'edge', 'seat', 'entire', 'time']\n",
      "Review 2: ['restaurant', 'best', 'pasta', 'ever', 'service', 'quick', 'ambiance', 'could', 'better']\n",
      "Review 3: ['disappointed', 'phone', 'battery', 'life', 'drain', 'way', 'fast', 'camera', 'quality', 'gr8', 'give']\n"
     ]
    }
   ],
   "source": [
    "nonum_reviews = [nonum_rev(tokens) for tokens in lemmatized_reviews]\n",
    "\n",
    "print(\"\\nReview with handled Numbers: \")\n",
    "for i, tokens in enumerate(nonum_reviews):\n",
    "    print(f\"Review {i+1}:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7827526",
   "metadata": {},
   "source": [
    "### Step 7. Removing Special Characters\n",
    "- Special characters like @, #, $, % don’t add value to most NLP tasks unless they represent specific information, like hashtags in social media.\n",
    "\n",
    "- Example:\n",
    "    - Text: \"The cost is $500 #expensive!\"\n",
    "\n",
    "    - Without special characters: \"The cost is 500 expensive\"\n",
    "\n",
    "Removing special characters is particularly important when dealing with formal text documents, but it might not always be desirable in social media data, where hashtags or mentions could carry important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cd62843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_char(tokens):\n",
    "    clean_tokens = [re.sub(r'[^A-Za-z0-9\\s]', '', token) for token in tokens ]\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e236316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Review:\n",
      "Review 1: ['movie', 'fantastic', 'storyline', 'kept', 'edge', 'seat', 'entire', 'time']\n",
      "Review 2: ['restaurant', 'best', 'pasta', 'ever', 'service', 'quick', 'ambiance', 'could', 'better']\n",
      "Review 3: ['disappointed', 'phone', 'battery', 'life', 'drain', 'way', 'fast', 'camera', 'quality', 'gr8', 'give']\n"
     ]
    }
   ],
   "source": [
    "no_specialchar_reviews = [remove_special_char(tokens) for tokens in nonum_reviews]\n",
    "\n",
    "print(\"\\nFinal Review:\")\n",
    "for i, tokens in enumerate(no_specialchar_reviews):\n",
    "    print(f\"Review {i+1}:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a8d9e5",
   "metadata": {},
   "source": [
    "### Step 8. Text Normalization\n",
    "- Inconsistent spelling or informal language use can cause confusion in NLP models. Normalization helps by converting variations of a word to a common form.\n",
    "\n",
    "- Example:\n",
    "    - Text: \"u r gr8!\"\n",
    "\n",
    "    - Normalized: \"you are great!\"\n",
    "\n",
    "- This is particularly useful when working with user-generated content, such as social media posts or customer feedback, where informal language is commo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29c0561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def normal_tokens(tokens):\n",
    "    normalized_text = [unicodedata.normalize('NFKD', token).encode('ascii', 'ignore').decode('utf-8') for token in tokens]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6068ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalized Reviews:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18860/3576464135.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nNormalized Reviews:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnormalized_reviews\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Review {i+1}:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "normalized_reviews = [normal_tokens(tokens) for tokens in no_specialchar_reviews]\n",
    "\n",
    "print(\"\\nNormalized Reviews:\")\n",
    "for i, tokens in enunormalized_reviews:\n",
    "    print(f\"Review {i+1}:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4d46d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
